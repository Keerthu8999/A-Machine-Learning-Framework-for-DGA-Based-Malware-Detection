% T behis is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.20 of 2017/10/04

%
\documentclass[runningheads]{llncs}
\documentclass{article}
%
\usepackage{amsmath}
\usepackage{booktabs} % For pretty tables
\usepackage{caption} % For caption spacing
\usepackage{subcaption} % For sub-figures
\usepackage{graphicx}
\usepackage{epstopdf}
\epstopdfDeclareGraphicsRule{.tif}{png}{.png}{convert #1 \OutputFile}
\AppendGraphicsExtensions{.tif}
\usepackage{pgfplots}
\usepackage[all]{nowidow}
\usepackage[utf8]{inputenc}
\usepackage{tikz}
\usetikzlibrary{er,positioning,bayesnet}
\usepackage{multicol}
\usepackage{algpseudocode,algorithm,algorithmicx}
\usepackage{minted}
\usepackage{hyperref}
\usepackage[inline]{enumitem} 
\usepackage{listings}
\usepackage{color}
\usepackage{acronym}
\usepackage{array}
\usepackage{wrapfig}
\usepackage{xcolor}


% Horizontal lists
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following line
% to display URLs in blue roman font according to Springer's eBook style:
% \renewcommand\UrlFont{\color{blue}\rmfamily}
\setcounter{secnumdepth}{4}
\newcommand{\card}[1]{\left\vert{#1}\right\vert}
\newcommand*\Let[2]{\State #1 $\gets$ #2}
\definecolor{blue}{HTML}{1F77B4}
\definecolor{orange}{HTML}{FF7F0E}
\definecolor{green}{HTML}{2CA02C}

\pgfplotsset{compat=1.14}

\renewcommand{\topfraction}{0.85}
\renewcommand{\bottomfraction}{0.85}
\renewcommand{\textfraction}{0.15}
\renewcommand{\floatpagefraction}{0.8}
\renewcommand{\textfraction}{0.1}
\setlength{\floatsep}{3pt plus 1pt minus 1pt}
\setlength{\textfloatsep}{3pt plus 1pt minus 1pt}
\setlength{\intextsep}{3pt plus 1pt minus 1pt}
\setlength{\abovecaptionskip}{2pt plus 1pt minus 1pt}

\begin{document}
%
\title{A Machine Learning Framework for \acshort{DGA} Based Malware Detection}
%
%\titlerunning{Abbreviated paper title}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{
G P Akhila\inst{1} \and
R Gayathri\inst{2} \and
S Keerthana \inst{3} \and
Dr Angelin Gladston  \inst{4}} 
%
%\authorrunning{F. Author et al.}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%

\institute{Department of Computer Science and Engineering, College of Engineering Guindy, Anna University\\
\email{akhilapadmanaban@gmail.com}\\  \and
Department of Computer Science and Engineering, College of Engineering Guindy, Anna University\\
\email{iamanifs@gmail.com} \\  \and
Department of Computer Science and Engineering, College of Engineering Guindy, Anna University\\
\email{keerusuja@gmail.com}  \and
Department of Computer Science 
and Engineering, College of Engineering Guindy, Anna University
\email{angel@cs.annauniv.edu}\\
}
%
\maketitle              % typeset the header of the contribution
%
\begin{abstract}
Real-time detection of domain names that are generated using the Domain Generation Algorithm (\acshort{DGA}s) is a challenging cyber security challenge. Traditional malware control methods, such as blacklisting, are insufficient to handle \acshort{DGA} threats. In this paper, a machine learning framework for identifying and detecting \acshort{DGA} domains is proposed to alleviate the threat. The proposed machine learning framework consists of a two-level model. In the two-level model, the \acshort{DGA} domains are classified apart from normal domains and then the clustering method is used to identify the algorithms that generate those \acshort{DGA} domains. 

\keywords{\acshort{DGA} \and Machine Learning \and Malware \and \acshort{DBSCAN} \and jaccard-index \and \acshort{GBT} \and \acshort{LR} \and J48 \and n-grame \and Entropy \and \acshort{OPTICS}.}
\end{abstract}
%
%
%

\section*{Abbreviations}

\begin{acronym}[DBSCAN] % Give the longest label here so that the list is nicely aligned
\color{blue}
\acro{DGA}{Domain generation algorithm}
\acro{DBSCAN}{Density-based spatial clustering of applications with noise}
\acro{OPTICS}{Ordering Points to Identify the Clustering Structure}
\acro{DNS}{Domain Name Server}
\acro{LSTM}{Long short-term memory}
\acro{LMS}{Longest Meaningful String }
\acro{SVM}{Support Vector Machine}
\acro{LR}{Logistic Regression}
\acro{GBT}{Gradient Boosting Tree}
\acro{RF}{Random Forest}
\end{acronym}
\section{Introduction}

Malware attackers attempt to infiltrate layers of protection and defensive solutions, resulting in threats on a computer network and its assets. Anti-malware software have been widely used in enterprises for a long time since they can provide some level of security on computer networks and systems to detect and mitigate malware attacks. However, many anti-malware solutions typically utilize static string matching approaches, hashing schemes, or network communication white-listing. These solutions are too simple to resolve sophisticated malware attacks, which can hide communication channels to bypass most detection schemes by purposely integrating evasive techniques. The issue has posed a serious threat to the security of an enterprise and it is also a grand challenge that needs to be addressed.

\acshort{DGA} are algorithms seen in various families of malware that are used to periodically generate a large number of domain names that can be used as rendezvous points with their command and control servers. The large number of potential rendezvous points makes it difficult for law enforcement to effectively shut down botnets, since infected computers will attempt to contact some of these domain names every day to receive updates or commands.

The research problem is  to accurately identify and cluster domains that originate from known \acshort{DGA}-based techniques where the target is to develop a security approach that autonomously mitigates network communications to unknown threats in a sequence.{\color{blue}{\cite{paperone}}}

The main contributions of this paper are summarized here.
1) A machine learning framework is proposed to perform \acshort{DGA} detection. First, the \acshort{DGA} domains are distinguished from normal domains and then the generation algorithm for those \acshort{DGA} domains are identified.
2) A two-level model consisting of classification and clustering is proposed to first classify \acshort{DGA} domain names and then cluster the \acshort{DGA}s to groups of different \acshort{DGA}s. The first-level model (also referred to the classification model) in the framework can provide a high accurate classification. In the second model, \acshort{DGA} domain names is clustered into different groups using an unsupervised \acshort{DBSCAN} algorithm and \acshort{OPTICS} algorithm, where \acshort{DBSCAN} represents Density-Based Spatial Clustering of Applications with Noise and \acshort{OPTICS} represents Ordering Points to Identify the Clustering Structure . By using the proposed two-level classification and clustering, the accuracy of identifying certain domain names from a \acshort{DGA} are improved. This is a fundamental requirement for future prediction of malicious attacks.


The rest of this paper is organised as follows. Section 2 gives an overview of existing methods. Section 3 involves data collection, describe the model and show how it can efficiently be used . Section 4 includes evaluation of the machine learning framework. Finally, Section 5 concludes the studies.


\section{Related work}

As the Internet has become widely distributed, it is very vulnerable to malware hazards \cite{papertwo}, \cite{paperthree}. Malware attackers can choose different targets or cyber-physical devices and attack them like mobile devices and connected vehicles. Many of the targets the threat actor attack are susceptible to malware attacks due to mismanagement issues, poor patching behaviors, and dangerous 0-day attacks \cite{paperfour}.

To differentiate \acshort{DGA} domain names from normal domain names, researchers have discovered that \acshort{DGA}-generated domain names contain significant features \cite{paperfive}. Therefore, many studies aim to target blocking those \acshort{DGA} domain names as {\color{blue}{a}} defense approach \cite{papersix}, \cite{paperseven}. The \acshort{DGA} that generates the domain fluxing botnet needs to be known so that the countermeasures are taken. Several studies have looked at understanding and reverse engineering the inner work- ings of botnets \cite{papereight}–\cite{thirteen}. Barabosch et al. \cite{fifteen} proposed an automatic method to extract \acshort{DGA} from current malware.

Their study focused on domain fluxing malware and relied on the binary extraction for \acshort{DGA}. Their approach is only effective for certain types of malware \cite{sixteen}. Besides blocking and extracting \acshort{DGA}s from normal domains, a further study has been explored based on the features of \acshort{DGA} domain names \cite{seventeen}.

Since the \acshort{DGA} domain names are usually randomly generated, the lengths of \acshort{DGA} domains are very long. Such a feature can be used to detect \acshort{DGA} domains. That is, shorter \acshort{DGA} domain names are more difficult to be detected. This is because most normal domains are tend to be short. Ahluwalia et al. \cite{eighteen} proposed a detection model that can dynamically detect \acshort{DGA} domains. They apply information theoretic features based on a domain length threshold. Their approach can dynamically detect the \acshort{DGA} domains with any length. Many other studies have been done on \acshort{DGA} detection based on the \acshort{DGA} domain features.

Ma et al. \cite{nineteen} proposed a lightweight approach to detect \acshort{DGA} domains based on URLs using both lexical and host- based features. They consider the lexical features of the URL such as length, number of dots, and special characters in the URL path. Antonakakis et al. \cite{twenty} proposed a novel detection system, called Pleiades. They extracted a number of statis- tical features related to the NXDOMAIN strings, including distribution of n-grams. Wang and Shirley \cite{twone} proposed using word segmentation to derive tokens from domain names to detect malicious domains. The proposed feature space includes the number of characters, digits, and hyphens. Simi- lar to Ma et al. \cite{nineteen}, McGrath and Gupta \cite{twtwo} also took a close look at phishing URLs and found that the phishing URLs and \acshort{DGA} domains had different characteristics when compared with normal domains and URLs. Therefore, they proposed a model for detecting \acshort{DGA} domains based on domain length comparison and character frequencies of English language alphabets. The similar approach based on \acshort{DGA} features can be found in \cite{papersix} and \cite{twthree}–\cite{twfive}.

In order to classify \acshort{DGA} domain names, Schiavoni et al. \cite{thirty} proposed a feasible approach for characterizing and clustering \acshort{DGA}-generated domains according to both linguistic and DNS features. In the study, they have assumed that \acshort{DGA} domains have groups of very significant characters from normal domains. By grouping domains according to their features, the authors applied a machine learning classifier to distinguish \acshort{DGA} domains from normal domains easily. Several machine-learning techniques have been studied to classify malicious codes. They include neural networks, support vector machines (SVM) and boosted classifiers \cite{twsix}. There are also several studies aiming to predict \acshort{DGA} domain names from historical \acshort{DGA} domains \cite{twseven}.

Woodbridge et al. \cite{tweight} used DNS queries to find the pattern of different families of \acshort{DGA}s. Their approach does not need a feature extraction step. Instead, it leverages  \acshort{LSTM} networks for real-time \acshort{DGA} prediction. Their approach can be easily implemented by using the open source tools. Similar to Woodbridge et al. \cite{tweight}, Xu et al. \cite{twnine} checked DNS similarity and pattern to predict future \acshort{DGA} domains. Their approach is effective for some \acshort{DGA}s.
In  \cite{fourteen}, they have proposed a machine learning framework that contains a blacklist and a two-level classification and clustering model to detect \acshort{DGA} domains. \\

Table 1 shows the comparison table between the existing work and the proposed work.

\begin{table}[h!]
\color{blue}
\begin{center}
\caption{Comparison Table.}
    \label{tab:table1}
\begin{tabular}{ | m{14em} | m{20em} | } 

\hline
Existing Work & Proposed Work \\
  \hline
  The system being analysed from existing works have used a clustering algorithm namely \acshort{DBSCAN}, which is density based, to group the various DGA domain names under their corresponding category. & The system being proposed here has used another density based clustering algorithm called \acshort{OPTICS}, which reduces noise in the clusters formed. For an automated clustering, \acshort{OPTICS} functions well since it can be run with infinite eps. It eases the work of a programmer by automating minpts and eps selection. \\ 
  \hline
   
  \hline
\end{tabular}
\end{center}
\end{table}
\section{Methodology}

\subsection{Threat Intelligence Feed {\color{blue}{and}} Ongoing Threat Data} \label{sec:dataset}

Malicious domain names generated from \acshort{DGA} are found in many examples that can be found from a simple Google search and Github. However, sophisticated threat actors purposely create tailored \acshort{DGA} to evaluate current detection systems. Using real-time active malicious domains derived from \acshort{DGA}s on the public Internet measures the accuracy of the proposed approach. The dataset that has been used is \emph{\acshort{DGA}-feed.txt} and \emph{alexa-top-1m.csv}, which is best suited for classification. The dataset \emph{\acshort{DGA}-feed.txt} contains the probable malicious domains and the dataset \emph{alexa-top-1m.csv} contains the first one million normal domain names. The files are fused to create a threat data feed that is fed into the following system. They have two fields domain name and the label. 

\subsection{Machine Learning Framework} \label{sec:ML framework}

A machine learning framework built for the detection of \acshort{DGA} based domains has  three important steps. The DNS queries are passed as input to the system followed by the processes. The process components includes : (1) A domain-request packet filter to get domain names and then store them in a dynamic blacklist. If the input is a known domain, the steps (2j48) - (3) are skipped and the output is directly obtained; else, the second component is used. (2) The features are extracted from a domain name with a feature extractor. (3) At the last step, the \acshort{DGA} domains are distinguished from non-\acshort{DGA} domains using a classification model and the second-level clustering to group similar \acshort{DGA} domains. After the domain name goes through the process step, this domain is appended to the dynamic blacklist. The overall block diagram is explained in Figure ~\ref{fig:block}
\begin{figure}[htb!]
\centering
\scalebox{0.5}{\includegraphics[width=2\textwidth,height=3\textheight,keepaspectratio]{figures/block600.tif}}
\caption{\label{fig:block}Overall block diagram}
\end{figure}

\subsubsection{Dynamic Blacklist \vspace{2mm}\\}
\noindent 
The domain names are the only information we need to perform classification and prediction in the following steps. A domain-request packet filter is used to filter out the trivial information, which is useless in the experiment involved, collected from the raw data. After filtering, only domain names are left from the step. In this process, a filtering method called Gruber Regex pattern filter is used. All the network traffic undergoes this filtering process.{\color{blue}{ By treating all URLs through the filter, about one-third of DGA names generated can be identified before passing through the machine learning model. This reduces the number of domain names to be tested and hence reduces the burden on the classifier. Gruber Regex Pattern has the ability to handle the URL properly other than other regex patterns which works just on simple strings. The advantages of using it is it does a better job with URLs containing parentheses,accepts URLs of the form x-dot-x-slash-x}}. The filtered domain names are stored in the dynamic blacklist , which is initially empty and will be updated dynamically and then sent to the feature extractor in next step. The dynamic blacklist can help to reduce unnecessary calculation, if a domain can be found in the dynamic blacklist, it is sent directly go to the output.
\subsubsection{Feature Extractor \vspace{2mm}\\}
\noindent
The feature extractor is used to extract features from the domain names filtered in the first component. The Feature extractor works on the PCA -Principal Component Analysis and the nine features are extracted with the help of n-gram. Each domain name is considered as a string. To efficiently classify domains, linguistic features are used. There are nine linguistic features: Length, Entropy, Unique Character rate and Vowel count, Meaningful WordRatio, Percentage of Numerical Characters, Pronounceability Score, Percentage of the Length of the \acshort{LMS}, and Levenshtein Edit Distance, Jaccard index.
The detailed description and calculation of each linguistic feature are given as follow:
\\\\
\begin{itemize}
\item \textbf{Length}: The length of the domain name $\mid$ d $\mid$ is a linguistic feature taken into account. {\color{blue}{The purpose of using length as a feature is, ordinary domains will have a a shorter length than \acshort{DGA} domains. If length of a domain is high, then it has a high probability of being a \acshort{DGA} Domain. }} \\

\item  \textbf{Entropy}: In information theory, entropy is a measure of the uncertainty or randomness associated with a variable. {\color{blue}{DGA domains are associated with high randomness. Hence, this feature could be used to calculate the entropy.}}
\begin{lstlisting}[language=Python]
p, lns = Counter(domain), float(len(domain))
entropy = (-sum(count/lns * math.log(count/lns, 2)
                       for count in p.values()))
                       
\end{lstlisting}


\item  \textbf{Unique character rate and Vowel count}: This feature gives an insight into the characteristics of the domain name based on the rate of unique characters. \acshort{DGA} domain names are likely to have repetition of characters and hence a lower unique character rate identifies such a domain name.
\begin{lstlisting}[language=Python]
unique_len = len(set(domain))
unique_rate = (unique_len+0.0)/len(domain)

vowel_count = len(re.findall(r'[aeiou]',domain.lower()))
vowel_rate = (0.0+vowel_count)/len(domain)

\end{lstlisting}

\item  \textbf{Meaningful Word Ratio}: The feature measures the ratio of number of meaningful words in a string (domain name) to the string length. This feature compares all possible sub-strings of the domain name with a standard list of words and calculates a ratio. {\color{blue}{\acshort{DGA} domains are known to have lesser number of meaningful words compared to legitimate domain names.}}
\begin{lstlisting}[language=Python]
english_vocab = set(line.strip() 
                    for line in open('wordlist.txt','r'))
#wordlist.txt contains a list of meaningful
#English dictionary words
for st in self._domain_list:
    tot=0.0
    all_words = {st[i:j + i] for j in range(2, len(st))
                            for i in range(len(st)- j + 1)}
    int_words = all_words.intersection(english_vocab)
    for i in int_words:
        if not any([i in sub_str for sub_str in int_words 
                                    if i != sub_str]):
            tot+=len(i)
    meaningful_word_ratio = tot/len(st)
    
\end{lstlisting}

\item  \textbf{Pronounceability Score}: A word can be pronounced by a likely combinations of phonemes. The score says about the words that can be pronounced. With a higher pronounceability score, the word is more pronounceable. The malicious \acshort{DGA} domain names have a higher probability of not being pronounceable or have a low pronounceability score. By extracting the n-gram score of the domain name, it is compared with the n-gram lookup table and the pronounceability score is obtained. Sub-strings of length l $\epsilon$ 2,3 are chosen for the computation and the results are counted with their occurrences in the English n-gram frequency text.\\

\item  \textbf{Percentage of Numerical Characters}: This feature measures the percentage of numerical characters in a string. A \acshort{DGA} will relatively have higher numerical characters than a normal one.\\

\item  \textbf{Percentage of the Length of \acshort{LMS}}: This feature is to measure the length of the longest meaningful string in a domain name. A \acshort{DGA} Domain will have less length where as a normal dmain's length will be more. \\

\item  \textbf{Levenshtein Edit Distance}: It measures the minimum number of single-character edits between a current domain and its previous domain in a stream of DNS queries received by the server. The Levenshtein distance is calculated based on a domain and its predecessor. For example, given two strings ‘‘test’’ and ‘‘task,’’ the Levenshtein Edit Distance between them is 2 because the characters that need to be edited are e to a and t to k. Another example is that for ‘‘word’’ and ‘‘world’’, the Levenshtein Edit Distance is 1 because we just need to add a l after the r. \cite{thone}\\

\item  \textbf{Jaccard index}: The jaccard measurement emphasizes similarity between consecutive domain names in the traffic, and is formally defined as the size of the intersection divided by the size of the union of the sample sets. This feature calculates average jaccard index score for a domain name by measuring the similarity of the domain names with the known domain names list.
\begin{lstlisting}[language=Python]
abs_intersection = np.intersect1d(list(domain_alpha),
                        list(domain_beta)).shape[0]
abs_union = np.union1d(list(domain_alpha), 
                        list(domain_beta)).shape[0]
jaccard_index = abs_intersection/abs_union*1.0
\end{lstlisting}
\end{itemize}

\subsubsection{Classification and Clustering \vspace{2mm} \\}
A two-level ML model is used for understanding \acshort{DGA} domains. The ML framework consists of a classification model for preliminary distinguishing and further a clustering model for grouping \acshort{DGA} domains with similar characteristics. In the classification, machine learning classifiers are used to distinguish \acshort{DGA} domains from normal domains in the set.
{\color{blue}{Each of the features extracted is assigned a weight and the model uses the features from previous section to calculate probabilities of the binary outcome and also sets a threshold to classify. It then classifies the domain name into one of the two categories (either DGA or normal) based on the calculated probability}}. Only the domains identified as \acshort{DGA} based domains are sent to the second-level clustering. Clustering techniques like \acshort{DBSCAN} and \acshort{OPTICS} are used to group the classified \acshort{DGA} domains into relative similar groups.

\paragraph{First level Classification \vspace{2mm} \\}
In the first-level classification, the features from the feature extractor are used and tested with different machine learning classifiers including
\begin{itemize}
\item \textbf{Decision Tree-J48}: C4.5 is an algorithm used to generate a decision tree developed by Ross Quinlan. C4.5 is an extension of Quinlan's earlier ID3 algorithm. The decision trees generated by C4.5 can be used for classification, and for this reason, C4.5 is often referred to as a statistical classifier. J48 is an open source Java implementation of the C4.5 algorithm in the Weka data mining tool. \cite{thtwo}\\

\item \textbf{SVM}: A SVM is a discriminative classifier formally defined by a separating hyperplane. In other words, given labeled training data (supervised learning), the algorithm outputs an optimal hyperplane which categorizes new examples.\cite{ththree}\\

\item \textbf{\acshort{LR}}:  The logistic regression is a predictive analysis. \acshort{LR} is used to describe data and to explain the relationship between one dependent binary variable and one or more nominal, ordinal, interval or ratio-level independent variables.\cite{thfour}\\

\item \textbf{\acshort{GBT}}: Gradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees.\cite{thfive}\\

\item \textbf{\acshort{RF}}: Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks that operates by constructing a multitude of decision trees \cite{thsix} at training time and outputting the class that is the mode of the classes or mean prediction of the individual trees. \cite{thseven}\\
\end{itemize}
With these the best classifier is identified. It is noted that, initially J48 classifies the best though with the increase in number of test cases for training, the \acshort{GBT} classifier has the highest accuracy.

\paragraph{Second level clustering \vspace{2mm}\\}
In the second-level clustering, we apply the \acshort{DBSCAN} and \acshort{OPTICS} algorithm. \acshort{DBSCAN} and \acshort{OPTICS} are one of the most common clustering algorithms and also most cited in scientific literature.\\

\textbf{\acshort{\acshort{DBSCAN}}}, a data clustering algorithm, uses set of rules to find clusters primarily based on density and doesn’t involve the use of parameters. The \acshort{DBSCAN} algorithm groups the closely packed points together. It also represents outliers as points that are alone and lie in the lower density area (the nearest neighbors being very far from the point considered). The pseudocode for \acshort{DBSCAN} algorithm is detailed below {\color{blue}{\cite{theight}:}}

\begin{lstlisting}[caption=Pseudocode of \acshort{DBSCAN} algorithm]
DBSCAN(DB, distFunc, eps, minPts) {
   C = 0                                                  /* Cluster counter */
   for each point P in database DB {
      if label(P) ≠ undefined then continue               
      Neighbors N = RangeQuery(DB, distFunc, P, eps)      
      if |N| < minPts then {                              
         label(P) = Noise                                 
         continue
      }
      C = C + 1                                          
      label(P) = C                                       
      Seed set S = N \ {P}                                
      for each point Q in S {                             
         if label(Q) = Noise then label(Q) = C          
         if label(Q) ≠ undefined then continue          
         label(Q) = C                                    
         Neighbors N = RangeQuery(DB, distFunc, Q, eps)  
         if |N| ≥ minPts then {                          
            S = S ∪ N                                    
         }
      }
   }
}
\end{lstlisting}

\textbf{\acshort{OPTICS}} algorithm uses a set of rules in spatial statistics to find clusters primarily based on density. \acshort{OPTICS} uses the same concept as \acshort{DBSCAN}, but this algorithm handles a major flaw of the \acshort{DBSCAN} algorithm : the issue of finding out meaningful clusters within varying density data. Spatially closest data points are treated as neighbors are they are ordered in a linear fashion. A distance parameter for a particular point defines the acceptable density for a cluster to include both the points in the same cluster. The pseudocode of \acshort{OPTICS} algorithm is as follows:{\color{blue} \cite{thnine}}}\\
\begin{lstlisting}[caption=Pseudocode of \acshort{OPTICS} algorithm]
 OPTICS(DB, eps, MinPts)
    for each point p of DB
       p.reachability-distance = UNDEFINED
    for each unprocessed point p of DB
       N = getNeighbors(p, eps)
       mark p as processed
       output p to the ordered list
       if (core-distance(p, eps, Minpts) != UNDEFINED)
          Seeds = empty priority queue
          update(N, p, Seeds, eps, Minpts)
          for each next q in Seeds
             N` = getNeighbors(q, eps)
             mark q as processed
             output q to the ordered list
             if (core-distance(q, eps, Minpts) != UNDEFINED)
                update(N`, q, Seeds, eps, Minpts)
\end{lstlisting}


Only the \acshort{DGA} domains obtained from the first-level classification will be used for clustering. The overall machine learning framework is explained in Figure ~\ref{fig:framework} 

\begin{figure}[H]
\centering
\scalebox{0.5}{\includegraphics[width=2\textwidth,height=3\textheight,keepaspectratio]{figures/framework600.tif}}
\caption{\label{fig:framework}ML framework}
\end{figure}

\section{Results}

To find the best model for the first-level classification,we test 5 different machine learning models, \acshort{J48},\acshort{SVM},  \acshort{GBT}, and \acshort{RF}. We perform the 10-fold cross-validation on these machine learning models.



\begin{figure}[H]
\centering
\scalebox{0.5}{\includegraphics[width=2\textwidth,height=3\textheight,keepaspectratio]{figures/result1300.tif}}
\caption{\label{fig:result1}Features extracted from the input dataset}
\end{figure}

\begin{figure}[H]
\centering
\scalebox{0.5}{\includegraphics[width=2\textwidth,height=3\textheight,keepaspectratio]{figures/result2300.tif}}
\caption{\label{fig:result2}Classification models that shows the output after the first level classification, using the 5 different machine learning models. From Fig 4 it’s evident that the J48 model gives the highest accuracy when compared to the other models as cited in the paper.}
\end{figure}

\begin{figure}[H]
\centering
\scalebox{0.5}{\includegraphics[width=2\textwidth,height=3\textheight,keepaspectratio]{figures/result3300.tif}}
\caption{\label{fig:result3}Input for clustering i.e the output after executing the classification model,where all the features are extracted and is of the format .npy.}
\end{figure}

\begin{figure}[H]
\centering
\scalebox{0.5}{\includegraphics[width=2\textwidth,height=3\textheight,keepaspectratio]{figures/result4300.tif}}
\caption{\label{fig:result4}Clustered data demonstrating the output after clustering into various domain names. Different colors of clustering indicates different domains where they belong to.}
\end{figure}

\begin{figure}[H]
\centering
\scalebox{0.5}{\includegraphics[width=2\textwidth,height=3\textheight,keepaspectratio]{figures/result5300.tif}}
\caption{\label{fig:result5}Model extracting features demonstrates the output that extracts the features that has been used in the model. The first set employs the training set(140000), whereas the second set uses the testing set(18755).}
\end{figure}

\section{Evaluation and Result Analysis}

The models are tested against several instances of training and testing set  for the chosen models - \acshort{SVM}, \acshort{LR}, \acshort{GBT} classifier, RF and Decision tree. The F1 score, harmonic mean of the precision and recall (where an F1 score reaches its best value at 1 and worst at 0) and the accuracy of the ML models implemented are tabulated against the number of samples in the train and test split.

\begin{table}[h!]
    \begin{center}
    \caption{Metrics of classification}
    \label{tab:table1}
    \setlength{\tabcolsep}{5pt}
    \begin{tabular}{1|2|3|4|5|6|7}
      \textbf{Train} & \textbf{Test} & \textbf{SVM} & \textbf{\acshort{LR}} & \textbf{\acshort{GBT}} & \textbf{\acshort{RF}} & \textbf{J48}\\
      \hline
       \multirow{}{}{1000 & 500} & acc=0.948 & acc=0.974 & acc=0.974 & acc=0.954 & acc=0.974\\
      & & f1=0.9733 & f1=0.986 & f1=0.986 & f1=0.976 & f1=0.9864\\ 
      \hline
      \multirow{}{}{5000 & 1500} & acc=0.952 & acc=0.962 & acc=0.976 & acc=0.949f & acc=0.966f\\
      & & f1=0.9752 & f1=0.9802 & f1=0.9877 & f1=0.9739 & f1=0.9820\\ 
      \hline
      \multirow{}{}{10000 & 5000} & acc=0.963 & acc=0.974 & acc=0.984 & acc=0.956 & acc=0.975f\\
      & & f1=0.98111 & f1=0.9868 & f1=0.992 & f1=0.9777 & f1=0.9869\\ 
      \hline
      \multirow{}{}{20000 & 7500} & acc=0.965 & acc=0.965 & acc=0.975f & acc=0.951 & acc=0.967\\
      & & f1=0.9821 & f1=0.9821 & f1=0.9886 & f1=0.975 & f1=0.9828\\ 
      \hline
      \multirow{}{}{50000 & 15000} & acc=0.971 & acc=0.966 & acc=0.976 & acc=0.947 & acc=0.97\\
      & & f1=0.9845 & f1=0.9834 & f1=0.9899 & f1=0.9744 & f1=0.9852\\ 
      \hline
      \multirow{}{}{140000 & 20000} & acc=0.981 & acc=0.969 & acc=0.982 & acc=0.949 & acc=0.975f\\
      & & f1=0.9901 & f1=0.9839 & f1=0.9903 & f1=0.9742 & f1=0.9873\\ 
      \hline
    \end{tabular}
    \end{center}
\end{table}

From the table~\ref{tab:table1}, it’s evident that the \acshort{GBT} model produces the highest accuracy for all the ratios of train : test , when compared to the other models. This changes the initial perception that J48 works best. \acshort{LR} is the second best to work with. SVM, RF, J48 the least preferred.

\begin{figure}[H]
\centering
\scalebox{0.5}{\includegraphics[width=2\textwidth,height=3\textheight,keepaspectratio]{figures/result6300.tif}}
\caption{\label{fig:result6}Performance of the Proposed Classifier}
\end{figure}

\begin{figure}[H]
\centering
\scalebox{0.5}{\includegraphics[width=2\textwidth,height=3\textheight,keepaspectratio]{figures/papermodel1300.tif}}
\caption{\label{fig:papermodel1}{\color{blue}{Performance of the Existing Classifier}}}
\end{figure}
\begin{figure}[H]
\centering
\scalebox{0.5}{\includegraphics[width=2\textwidth,height=3\textheight,keepaspectratio]{figures/comparison300.tif}}
\caption{\label{fig:result7}Comparison between the \acshort{GBT} Classifier’s performance}
\end{figure}
{\color{blue}{Figure 8 shows the performance of the proposed classifier, and Figure 9 shows the performance of the existing model for various classifiers adapted from \cite{paperone}. Figure 10 shows the comparison between the existing and the proposed system for the \acshort{GBT} Classifier, since highest accuracy has been achieved from \acshort{GBT}}}.
\begin{figure}[H]
\centering
\scalebox{0.5}{\includegraphics[width=2\textwidth,height=3\textheight,keepaspectratio]{figures/result300.tif}}
\caption{\label{fig:result}Clustered output for fixed eps and varying min_samples}
\end{figure}

\begin{figure}[H]
\centering
\scalebox{0.5}{\includegraphics[width=2\textwidth,height=3\textheight,keepaspectratio]{figures/result8300.tif}}
\caption{\label{fig:result8}Clustered output for fixed min_samples and varying eps.}
\end{figure}

\begin{figure}[H]
\centering
\scalebox{0.5}{\includegraphics[width=2\textwidth,height=3\textheight,keepaspectratio]{figures/result9300.tif}}
\caption{\label{fig:result9}Clustered output after implementing the \acshort{OPTICS} algorithm}
\end{figure}


The OPTICS seems to have produced better clustering results as seen in the figure~\ref{fig:result}. Figure~\ref{fig:result8}denotes a comparison between the two clustering algorithms - \acshort{DBSCAN} and \acshort{OPTICS}.

\begin{figure}[h]
\begin{subfigure}{0.5\textwidth}
\includegraphics[width=0.9\linewidth, height=5cm]{figures/result10300.tif} 
\label{fig:subim1}
\end{subfigure}
\begin{subfigure}{0.5\textwidth}
\includegraphics[width=0.9\linewidth, height=5cm]{figures/result11300.tif}
\label{fig:subim2}
\end{subfigure}
\caption{\acshort{OPTICS} vs \acshort{DBSCAN} Algorithm}
\label{fig:comp1}
\end{figure}



\begin{figure}[h]
\begin{subfigure}{0.5\textwidth}
\includegraphics[width=0.9\linewidth, height=5cm]{figures/result12300.tif} 
\label{fig:subim1}
\end{subfigure}
\begin{subfigure}{0.5\textwidth}
\includegraphics[width=0.9\linewidth, height=5cm]{figures/result13300.tif}
\label{fig:subim2}
\end{subfigure}
 
\caption{Domains and Labels of Corresponding Clusters.}
\label{fig:comp2}
\end{figure}

\section{Conclusion}

Detecting \acshort{DGA}s is a grand challenge in security areas. Blacklisting is good for handling static methods. However, \acshort{DGA}s are usually used by an attacker to communicate with variety of servers. They are dynamic, so simply using the blacklisting is not sufficient for detecting a \acshort{DGA}. The dynamically changing nature of the malicious domains needs to be addressed with an advanced system capable of detecting the history of the domain. The proposed machine learning framework consists of a dynamic blacklist, a feature extractor, a two-level machine learning model for classification and clustering. Thus, it is shown that the proposed framework can effectively extract domain name features as well as classify, cluster the malicious domains for a more specific detection.

%
% ---- Bibliography ----
%
% BibTeX users should specify bibliography style 'splncs04'.
% References will then be sorted and formatted in the correct style.
%
\bibliographystyle{splncs04}
\begin{thebibliography}{39}

\bibitem{paperone} 
Y. Li, K. Xiong, T. Chin and C. Hu. 
\textit{A Machine Learning Framework for Domain Generation Algorithm-Based Malware Detection} in IEEE Access, vol. 7, pp. 32765-32782, 2019.

\bibitem{papertwo} 
K. Xiong and X. Chen. 
\textit{Ensuring cloud service guarantees via service level agreement (SLA)-based resource allocation,} 
in Proc.IEEE 35th Int. Conf. Distrib. Comput. Syst. Workshops, ICDCS Workshops Jun./Jul. 2015, pp. 35–41.

\bibitem{paperthree} 
T. Chin and K. Xiong.
\textit{Dynamic generation containment systems (DGCS): A moving target defense approach,} 
in Proc. 3rd Int. Workshop Emerg. Ideas Trends Eng. Cyber-Phys. Syst. (EITEC), Apr. 2016,pp. 11–16, doi: 10.1109/EITEC.2016.7503690.

\bibitem{paperfour} 
K. Sornalakshmi.
\textit{Detection of DoS attack and zero day threat with SIEM,}
in Proc. IEEE Int. Conf. Intell. Comput. Control Syst. (ICICCS), Jun. 2017, pp. 1–7.


\bibitem{paperfive} 
S. Yadav and A. L. N. Reddy.
\textit{Winning with DNS failures: Strategies for faster botnet detection,} 
in Proc. Int. Conf. Secur. Privacy Commun. Syst. Berlin, Germany: Springer, 2011, pp. 446–459.

\bibitem{papersix} 
S. Yadav, A. K. K. Reddy, A. L. N. Reddy, and S. Ranjan.
\textit{Detecting algorithmically generated domain-flux attacks with DNS traffic analysis}. 
IEEE/ACM Trans. Netw., vol. 20, no. 5, pp. 1663–1677, Oct. 2012.

\bibitem{paperseven} 
F. Guo, P. Ferrie, and T.-C. Chiueh.
\textit{A study of the packer problem and its solutions,}
in Proc. Int. Workshop Recent Adv. Intrusion Detection. Berlin, Germany: Springer, 2008, pp. 98–115.

\bibitem{papereight} 
T. Holz et al.,
\textit{Measurements and mitigation of peer-to-peer-based botnets: A case study on storm worm,}
in Proc. LEET, vol. 8, no. 1, 2008, pp. 1–9.

\bibitem{papernine} 
R. Langner.
\textit{Stuxnet: Dissecting a cyberwarfare weapon}. 
IEEE Secur. Privacy, vol. 9, no. 3, pp. 49–51, May/Jun. 2011.

\bibitem{paperten} 
J. Stewart.
\textit{Inside the storm: Protocols and encryption of the storm botnet,}
in Proc. Black Hat Tech. Secur. Conf., New York, NY, USA, 2009.

\bibitem{eleven} 
H. S. Phillip Porras and V. Yegneswaran.
\textit{Conficker C P2P protocol and implementation}. 
2009.

\bibitem{twelve} 
B. Stone-Gross et al.,
\textit{Your botnet is my botnet: analysis of a botnet takeover,}
in Proc. 16th ACM Conf. Comput. Commun. Secur., 2009, pp. 635–647.

\bibitem{thirteen} 
L. Zhang, S. Yu, D. Wu, and P. Watters.
\textit{A survey on latest botnet attack and defense,}
in Proc. IEEE 10th Int. Conf. Trust, Secur. Privacy Comput. Commun. (TrustCom), Nov. 2011, pp. 53–60.


\bibitem{fourteen} 
T. Chin, K. Xiong, C. Hu, and Y. Li.
\textit{A machine learning framework for studying domain generation algorithm (\acshort{DGA})-based malware,}
in Proc. SecureComm, 2018, pp. 433–448.


\bibitem{fifteen} 
T. Barabosch, A. Wichmann, F. Leder, and E. Gerhards-Padilla.
\textit{Automatic extraction of domain name generation algorithms from current malware,} 
in Proc. NATO Symp. IST Inf. Assurance Cyber Defense, Koblenz,Germany, 2012, pp. 1–13.


\bibitem{sixteen} 
J. Gardiner and S. Nagaraja.
\textit{On the security of machine learning in malware C\&C detection: A survey}. 
ACM Comput. Surv., vol. 49, no. 3,pp. 59-1–59-39, Dec. 2016.


\bibitem{seventeen} 
M. Mowbray and J. Hagen.
\textit{Finding domain-generation algorithms bylooking at length distribution,} 
in Proc. IEEE Int. Symp. Softw. Rel.Eng. Workshops (ISSREW), Nov. 2014, pp. 395–400.

\bibitem{eighteen} 
A. Ahluwalia, I. Traore, K. Ganame, and N. Agarwal.
\textit{Detecting broad length algorithmic-ally generated domains,}
in Proc. Int. Conf. Intell., Secure, Dependable Syst. Distrib. Cloud Environ. Cham, Switzerland: Springer, 2017, pp. 19–34.

\bibitem{nineteen} 
 J. Ma, L. K. Saul, S. Savage, and G. M. Voelker.
\textit{Beyond blacklists:Learning to detect malicious Web sites from suspicious URLs,}
in Proc. 15th ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining, 2009, pp. 1245–1254.


\bibitem{twenty} 
M. Antonakakis et al.,
\textit{From throw-away traffic to Bots: Detecting the rise of \acshort{DGA}-based malware,}
in Proc. USENIX Secur. Symp., vol. 12, 2012, pp. 24–24.


\bibitem{twone} 
 W. Wang and K. Shirley. (2015).
\textit{Breaking bad: Detecting malicious domains using word segmentation}. 
[Online]. Available:  \texttt {https://arxiv.org/abs/1506.04111}.


\bibitem{twtwo} 
D. K. McGrath and M. Gupta.
\textit{Behind phishing: An examination of phisher modi operandi,}
in Proc. LEET, vol. 8, 2008, p. 4.

\bibitem{twthree} 
Y. He, Z. Zhong, S. Krasser, and Y. Tang.
\textit{Mining DNS for malicious domain registrations,}
in Proc. IEEE 6th Int. Conf. Collaborative Comput., Netw., Appl. Worksharing (CollaborateCom), Oct. 2010, pp. 1–6.

\bibitem{twfour} 
L. Bilge, E. Kirda, C. Kruegel, and M. Balduzzi.
\textit{EXPOSURE: Finding malicious domains using passive DNS analysis,}
in Proc. NDSS, 2011 pp. 1–17.


\bibitem{twfive} 
S. Srinivasan, S. Bhattacharya, and R. Chakraborty.
\textit{Segmenting Web-domains and hashtags using length specific models,}
in Proc. 21st ACM Int. Conf. Inf. Knowl. Manage., 2012, pp. 1113–1122.

\bibitem{twsix} 
A. Shabtai, R. Moskovitch, Y. Elovic, and C. Glezer.
\textit{Detection of malicious code by applying machine learning classifiers on static features: A state-of-the-art survey}. 
Inf. Secur. Tech. Rep., vol. 14, no. 1, pp. 16–29, 2009.


\bibitem{twseven} 
R. Sharifnya and M. Abadi.
\textit{A novel reputation system to detect \acshort{DGA}-based botnets,}
in Proc. IEEE 3rd Int. Conf. Comput. Knowl.Eng. (ICCKE), Oct./Nov. 2013, pp. 417–423.


\bibitem{tweight} 
J. Woodbridge, H. S. Anderson, A. Ahuja, and D. Grant. (2016).
\textit{Predicting domain generation algorithms with long short-term memory networks}. 
[Online]. Available:  \texttt {https://arxiv.org/abs/1611.00791}.

\bibitem{twnine} 
W. Xu, K. Sanders, and Y. Zhang.
\textit{We know it before you do: Predicting malicious domains,}
in Proc. Virus Bull. Conf., 2014, pp. 1–5.


\bibitem{thirty} 
S. Schiavoni, F. Maggi, L. Cavallaro, and S. Zanero.
\textit{Phoenix: \acshort{DGA}-based botnet tracking and intelligence,}
in Proc. Int. Conf. Detection Intrusions Malware, Vulnerability Assessment. Cham, Switzerland: Springer,2014, pp. 192–211.


\bibitem{thone} 
Levenshtein Distance
\\\href{https://en.wikipedia.org/wiki/Levenshtein_distance}{\texttt{https://en.wikipedia.org/wiki/Levenshtein\_distance}}

\bibitem{thtwo} 
J48 algorithm
\\\href{https://en.wikipedia.org/wiki/C4.5_algorithm}{\texttt{https://en.wikipedia.org/wiki/C4.5\_algorithm}}

\bibitem{ththree} 
Support Vector Machines
\\\href{https://en.wikipedia.org/wiki/Support-vector_machine
}{\texttt{https://en.wikipedia.org/wiki/Support\-vector\_machine
}}

\bibitem{thfour} 
Logistic Regression
\\\href{https://en.wikipedia.org/wiki/Logistic_regression}{\texttt{ https://en.wikipedia.org/wiki/Logistic\_regression}}



\bibitem{thfive} 
Gradient Boosting Tree
\\\href{https://en.wikipedia.org/wiki/Gradient_boosting}{\texttt{https://en.wikipedia.org/wiki/Gradient\_boosting}}

\bibitem{thsix} 
Decision Tree
\\\href{https://en.wikipedia.org/wiki/Decision_tree}{\texttt{https://en.wikipedia.org/wiki/Decision\_tree}}

\bibitem{thseven} 
 Random Forest
\\\href{https://en.wikipedia.org/wiki/Random_forest}{\texttt{https://en.wikipedia.org/wiki/Random\_forest}}

\bibitem{theight} 
 \acshort{DBSCAN}
\\\href{https://en.wikipedia.org/wiki/\acshort{DBSCAN}}{\texttt{https://en.wikipedia.org/wiki/\acshort{DBSCAN}}}

\bibitem{thnine} 
 \acshort{OPTICS}
\\\href{https://en.wikipedia.org/wiki/\acshort{OPTICS}_algorithm}{\texttt{https://en.wikipedia.org/wiki/\acshort{OPTICS}_algorithm}}

\end{thebibliography}

%
\end{document}
