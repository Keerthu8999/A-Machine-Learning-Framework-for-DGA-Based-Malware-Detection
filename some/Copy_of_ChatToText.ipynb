{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMrSvkWLOOP6khjS7oGreCO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Keerthu8999/A-Machine-Learning-Framework-for-DGA-Based-Malware-Detection/blob/master/Copy_of_ChatToText.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jtNrU3zeBHbH"
      },
      "outputs": [],
      "source": [
        "pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BartForConditionalGeneration, BartTokenizer, BartConfig\n",
        "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "wqC5rE20Bg58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your labeled dataset\n",
        "dataset = load_dataset(\"samsum\")"
      ],
      "metadata": {
        "id": "6QNTqwpdBmkb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece\n",
        "!pip install transformers==4.10.0\n"
      ],
      "metadata": {
        "id": "UE72RQm-GR7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"facebook/bart-large-cnn\"\n",
        "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
        "tokenizer = BartTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "8igk3GMlF_fi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the dataset\n",
        "def tokenize_function(examples):\n",
        "    #dialogue_text = examples[\"dialogue\"]\n",
        "    #print(f\"Original Dialogue Text: {dialogue_text}\")\n",
        "\n",
        "    # Tokenize the dialogue text\n",
        "    tokenized_inputs = tokenizer(\n",
        "        examples[\"dialogue\"],\n",
        "        return_tensors=\"pt\",\n",
        "        max_length=1024,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\"  # Add padding to ensure consistent input shape\n",
        "    )\n",
        "\n",
        "    # Print the tokenized input IDs (optional)\n",
        "    print(\"Tokenized Input IDs:\", tokenized_inputs[\"input_ids\"])\n",
        "\n",
        "    return tokenized_inputs\n",
        "\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "id": "auRjTBkRB6tB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip setuptools\n",
        "!pip install accelerate -U"
      ],
      "metadata": {
        "id": "QRUIo79oCL8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.10.2"
      ],
      "metadata": {
        "id": "T8qIq6PVE0U1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForSeq2Seq\n",
        "\n",
        "seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
      ],
      "metadata": {
        "id": "bEvAcrfTHYYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_examples_to_features(example_batch):\n",
        "    input_encodings = tokenizer(example_batch['dialogue'] , max_length = 1024, truncation = True )\n",
        "\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        target_encodings = tokenizer(example_batch['summary'], max_length = 128, truncation = True )\n",
        "\n",
        "    return {\n",
        "        'input_ids' : input_encodings['input_ids'],\n",
        "        'attention_mask': input_encodings['attention_mask'],\n",
        "        'labels': target_encodings['input_ids']\n",
        "    }\n",
        "\n",
        "dataset_samsum_pt = dataset.map(convert_examples_to_features, batched = True)"
      ],
      "metadata": {
        "id": "uOefDaD0HmJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "trainer_args = TrainingArguments(\n",
        "    output_dir='pegasus-samsum', num_train_epochs=1, warmup_steps=500,\n",
        "    per_device_train_batch_size=1, per_device_eval_batch_size=1,\n",
        "    weight_decay=0.01, logging_steps=10,\n",
        "    evaluation_strategy='steps', eval_steps=500, save_steps=1e6,\n",
        "    gradient_accumulation_steps=16\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "trainer = Trainer(model=model, args=trainer_args,\n",
        "                  tokenizer=tokenizer, data_collator=seq2seq_data_collator,\n",
        "                  train_dataset=dataset_samsum_pt[\"train\"],\n",
        "                  eval_dataset=dataset_samsum_pt[\"validation\"])\n",
        "\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "HkGcao-GCZSW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"samsum-model\")\n",
        "\n",
        "tokenizer.save_pretrained(\"tokenizer\")\n"
      ],
      "metadata": {
        "id": "db9Kcm8AEIub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_samsum = load_dataset(\"samsum\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"tokenizer\")\n",
        "sample_text = dataset_samsum[\"test\"][0][\"dialogue\"]\n",
        "\n",
        "reference = dataset_samsum[\"test\"][0][\"summary\"]"
      ],
      "metadata": {
        "id": "8g4P435tPxcU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "gen_kwargs = {\"length_penalty\": 0.8, \"num_beams\":8, \"max_length\": 128}\n",
        "\n",
        "pipe = pipeline(\"summarization\", model=\"samsum-model\",tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "D1XrHk94P68j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "BK1gg-rGQqPx",
        "outputId": "8fa46301-2448-4a9d-eb42-5932e27b05d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Hannah: Hey, do you have Betty's number?\\nAmanda: Lemme check\\nHannah: <file_gif>\\nAmanda: Sorry, can't find it.\\nAmanda: Ask Larry\\nAmanda: He called her last time we were at the park together\\nHannah: I don't know him well\\nHannah: <file_gif>\\nAmanda: Don't be shy, he's very nice\\nHannah: If you say so..\\nHannah: I'd rather you texted him\\nAmanda: Just text him ðŸ™‚\\nHannah: Urgh.. Alright\\nHannah: Bye\\nAmanda: Bye bye\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Dialogue:\")\n",
        "print(sample_text)\n",
        "\n",
        "\n",
        "print(\"\\nReference Summary:\")\n",
        "print(reference)\n",
        "\n",
        "\n",
        "print(\"\\nModel Summary:\")\n",
        "print(pipe(sample_text, **gen_kwargs)[0][\"summary_text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8IQ73IhQGw4",
        "outputId": "93edbe34-01fd-4a77-8bfa-3425f2bec292"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dialogue:\n",
            "Hannah: Hey, do you have Betty's number?\n",
            "Amanda: Lemme check\n",
            "Hannah: <file_gif>\n",
            "Amanda: Sorry, can't find it.\n",
            "Amanda: Ask Larry\n",
            "Amanda: He called her last time we were at the park together\n",
            "Hannah: I don't know him well\n",
            "Hannah: <file_gif>\n",
            "Amanda: Don't be shy, he's very nice\n",
            "Hannah: If you say so..\n",
            "Hannah: I'd rather you texted him\n",
            "Amanda: Just text him ðŸ™‚\n",
            "Hannah: Urgh.. Alright\n",
            "Hannah: Bye\n",
            "Amanda: Bye bye\n",
            "\n",
            "Reference Summary:\n",
            "Hannah needs Betty's number but Amanda doesn't have it. She needs to contact Larry.\n",
            "\n",
            "Model Summary:\n",
            "Amanda is looking for Betty's number. Larry called Betty the last time they were at the park together. Hannah doesn't know Larry well. Hannah wants Amanda to text him instead of asking Larry.   Â    and Amanda will text him. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lol = \"Person A: Hi there!\\nPerson B: Hello! How are you?\\nPerson A: I'm doing well, thanks. How about you?\\nPerson B: Not bad. I had a busy day at work.\\nPerson A: Oh, that sounds interesting. Tell me more.\\nPerson B: Well, I had a meeting with the client and then worked on some reports...\\nPerson A: Got it. Anything exciting happened?\\nPerson B: Not really, just the usual stuff.\\nPerson A: Alright. Let me know if there's anything new.\\nPerson B: Sure thing. Catch you later!\"\n",
        "print(\"Dialogue:\")\n",
        "print(lol)\n",
        "\n",
        "\n",
        "print(\"\\nReference Summary:\")\n",
        "print(reference)\n",
        "\n",
        "\n",
        "print(\"\\nModel Summary:\")\n",
        "print(pipe(lol, **gen_kwargs)[0][\"summary_text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhGk8RywRvMO",
        "outputId": "d6308556-3672-4afb-e6b9-901722fdbdfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dialogue:\n",
            "Person A: Hi there!\n",
            "Person B: Hello! How are you?\n",
            "Person A: I'm doing well, thanks. How about you?\n",
            "Person B: Not bad. I had a busy day at work.\n",
            "Person A: Oh, that sounds interesting. Tell me more.\n",
            "Person B: Well, I had a meeting with the client and then worked on some reports...\n",
            "Person A: Got it. Anything exciting happened?\n",
            "Person B: Not really, just the usual stuff.\n",
            "Person A: Alright. Let me know if there's anything new.\n",
            "Person B: Sure thing. Catch you later!\n",
            "\n",
            "Reference Summary:\n",
            "Hannah needs Betty's number but Amanda doesn't have it. She needs to contact Larry.\n",
            "\n",
            "Model Summary:\n",
            "Person B had a busy day at work. He had a meeting with the client and worked on some reports. Person A will let Person B know if there's anything new at work that he's doing. They will see each other later at Person A's place.\n"
          ]
        }
      ]
    }
  ]
}
